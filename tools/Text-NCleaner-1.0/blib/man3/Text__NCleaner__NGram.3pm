.\" Automatically generated by Pod::Man 2.25 (Pod::Simple 3.16)
.\"
.\" Standard preamble:
.\" ========================================================================
.de Sp \" Vertical space (when we can't use .PP)
.if t .sp .5v
.if n .sp
..
.de Vb \" Begin verbatim text
.ft CW
.nf
.ne \\$1
..
.de Ve \" End verbatim text
.ft R
.fi
..
.\" Set up some character translations and predefined strings.  \*(-- will
.\" give an unbreakable dash, \*(PI will give pi, \*(L" will give a left
.\" double quote, and \*(R" will give a right double quote.  \*(C+ will
.\" give a nicer C++.  Capital omega is used to do unbreakable dashes and
.\" therefore won't be available.  \*(C` and \*(C' expand to `' in nroff,
.\" nothing in troff, for use with C<>.
.tr \(*W-
.ds C+ C\v'-.1v'\h'-1p'\s-2+\h'-1p'+\s0\v'.1v'\h'-1p'
.ie n \{\
.    ds -- \(*W-
.    ds PI pi
.    if (\n(.H=4u)&(1m=24u) .ds -- \(*W\h'-12u'\(*W\h'-12u'-\" diablo 10 pitch
.    if (\n(.H=4u)&(1m=20u) .ds -- \(*W\h'-12u'\(*W\h'-8u'-\"  diablo 12 pitch
.    ds L" ""
.    ds R" ""
.    ds C` ""
.    ds C' ""
'br\}
.el\{\
.    ds -- \|\(em\|
.    ds PI \(*p
.    ds L" ``
.    ds R" ''
'br\}
.\"
.\" Escape single quotes in literal strings from groff's Unicode transform.
.ie \n(.g .ds Aq \(aq
.el       .ds Aq '
.\"
.\" If the F register is turned on, we'll generate index entries on stderr for
.\" titles (.TH), headers (.SH), subsections (.SS), items (.Ip), and index
.\" entries marked with X<> in POD.  Of course, you'll have to process the
.\" output yourself in some meaningful fashion.
.ie \nF \{\
.    de IX
.    tm Index:\\$1\t\\n%\t"\\$2"
..
.    nr % 0
.    rr F
.\}
.el \{\
.    de IX
..
.\}
.\"
.\" Accent mark definitions (@(#)ms.acc 1.5 88/02/08 SMI; from UCB 4.2).
.\" Fear.  Run.  Save yourself.  No user-serviceable parts.
.    \" fudge factors for nroff and troff
.if n \{\
.    ds #H 0
.    ds #V .8m
.    ds #F .3m
.    ds #[ \f1
.    ds #] \fP
.\}
.if t \{\
.    ds #H ((1u-(\\\\n(.fu%2u))*.13m)
.    ds #V .6m
.    ds #F 0
.    ds #[ \&
.    ds #] \&
.\}
.    \" simple accents for nroff and troff
.if n \{\
.    ds ' \&
.    ds ` \&
.    ds ^ \&
.    ds , \&
.    ds ~ ~
.    ds /
.\}
.if t \{\
.    ds ' \\k:\h'-(\\n(.wu*8/10-\*(#H)'\'\h"|\\n:u"
.    ds ` \\k:\h'-(\\n(.wu*8/10-\*(#H)'\`\h'|\\n:u'
.    ds ^ \\k:\h'-(\\n(.wu*10/11-\*(#H)'^\h'|\\n:u'
.    ds , \\k:\h'-(\\n(.wu*8/10)',\h'|\\n:u'
.    ds ~ \\k:\h'-(\\n(.wu-\*(#H-.1m)'~\h'|\\n:u'
.    ds / \\k:\h'-(\\n(.wu*8/10-\*(#H)'\z\(sl\h'|\\n:u'
.\}
.    \" troff and (daisy-wheel) nroff accents
.ds : \\k:\h'-(\\n(.wu*8/10-\*(#H+.1m+\*(#F)'\v'-\*(#V'\z.\h'.2m+\*(#F'.\h'|\\n:u'\v'\*(#V'
.ds 8 \h'\*(#H'\(*b\h'-\*(#H'
.ds o \\k:\h'-(\\n(.wu+\w'\(de'u-\*(#H)/2u'\v'-.3n'\*(#[\z\(de\v'.3n'\h'|\\n:u'\*(#]
.ds d- \h'\*(#H'\(pd\h'-\w'~'u'\v'-.25m'\f2\(hy\fP\v'.25m'\h'-\*(#H'
.ds D- D\\k:\h'-\w'D'u'\v'-.11m'\z\(hy\v'.11m'\h'|\\n:u'
.ds th \*(#[\v'.3m'\s+1I\s-1\v'-.3m'\h'-(\w'I'u*2/3)'\s-1o\s+1\*(#]
.ds Th \*(#[\s+2I\s-2\h'-\w'I'u*3/5'\v'-.3m'o\v'.3m'\*(#]
.ds ae a\h'-(\w'a'u*4/10)'e
.ds Ae A\h'-(\w'A'u*4/10)'E
.    \" corrections for vroff
.if v .ds ~ \\k:\h'-(\\n(.wu*9/10-\*(#H)'\s-2\u~\d\s+2\h'|\\n:u'
.if v .ds ^ \\k:\h'-(\\n(.wu*10/11-\*(#H)'\v'-.4m'^\v'.4m'\h'|\\n:u'
.    \" for low resolution devices (crt and lpr)
.if \n(.H>23 .if \n(.V>19 \
\{\
.    ds : e
.    ds 8 ss
.    ds o a
.    ds d- d\h'-1'\(ga
.    ds D- D\h'-1'\(hy
.    ds th \o'bp'
.    ds Th \o'LP'
.    ds ae ae
.    ds Ae AE
.\}
.rm #[ #] #H #V #F C
.\" ========================================================================
.\"
.IX Title "Text::NCleaner::NGram 3"
.TH Text::NCleaner::NGram 3 "2008-03-22" "perl v5.14.2" "User Contributed Perl Documentation"
.\" For nroff, turn off justification.  Always turn off hyphenation; it makes
.\" way too many mistakes in technical documents.
.if n .ad l
.nh
.SH "NAME"
Text::NCleaner::NGram \- Simple character\-level n\-gram language model
.SH "SYNOPSIS"
.IX Header "SYNOPSIS"
This module implements simple character-level n\-gram language models using
geometric interpolation for the history and add-one smoothing for unigrams.
It is intended for byte strings that consist mostly of \s-1ASCII\s0 characters and
will ignore any characters outside this range. The module will not work
properly with strings treated as Unicode data by the Perl interpreter; these
should be converted to \s-1UTF\-8\s0 or a suitable \s-1ISO\-8859\s0 encoding first (using the
\&\fBEncode\fR module).
.PP
.Vb 1
\&    use Text::NCleaner::NGram;
.Ve
.PP
\&\fB\s-1TODO:\s0 add brief usage example\fR
.SH "METHODS"
.IX Header "METHODS"
.ie n .IP "\fI\fI$model\fI\fR = \fBnew\fR Text::NCleaner::NGram \fI\fI$n\fI\fR;" 4
.el .IP "\fI\f(CI$model\fI\fR = \fBnew\fR Text::NCleaner::NGram \fI\f(CI$n\fI\fR;" 4
.IX Item "$model = new Text::NCleaner::NGram $n;"
Initialise new, untrained n\-gram model of order \fI\f(CI$n\fI\fR (history size is \fI\f(CI$n\fI\fR\-1).
Returns object of class \fBText::NCleaner::NGram\fR.
.ie n .IP "\fI\fI$model\fI\fR = \fBnew\fR Text::NCleaner::NGram \fI\fI$filename\fI\fR;" 4
.el .IP "\fI\f(CI$model\fI\fR = \fBnew\fR Text::NCleaner::NGram \fI\f(CI$filename\fI\fR;" 4
.IX Item "$model = new Text::NCleaner::NGram $filename;"
Load pre-trained n\-gram model from file \fI\f(CI$filename\fI\fR (must have been
serialised with \fBsave\fR method).
.ie n .IP "\fI\fI$model\fI\fR\->\fBsave\fR(\fI\fI$filename\fI\fR);" 4
.el .IP "\fI\f(CI$model\fI\fR\->\fBsave\fR(\fI\f(CI$filename\fI\fR);" 4
.IX Item "$model->save($filename);"
Save trained n\-gram model to portable disk file. The serialised file is Perl
code (generated by \fBData::Dumper\fR) which can be executed to recreate the
\&\fBText::NCleaner::NGram\fR object.
.ie n .IP "\fI\fI$model\fI\fR\->\fBdebug\fR(\fI\fI$level\fI\fR);" 4
.el .IP "\fI\f(CI$model\fI\fR\->\fBdebug\fR(\fI\f(CI$level\fI\fR);" 4
.IX Item "$model->debug($level);"
Enable / disable debugging messages.  Debugging level 1 performs additional
consistency checks and prints some diagnostic messages.  Debugging level 2
prints detailed information.
.ie n .IP "\fI\fI$model\fI\fR\->\fBset_q\fR(\fI\fI$q\fI\fR);" 4
.el .IP "\fI\f(CI$model\fI\fR\->\fBset_q\fR(\fI\f(CI$q\fI\fR);" 4
.IX Item "$model->set_q($q);"
Set interpolation factor \fI\f(CI$q\fI\fR of the n\-gram model, which must be in the range
\&\fI(0,1)\fR.  Values close to 1 correspond to strong smoothing (all history sizes
have equal weight) while values very close to 0 disable smoothing.
.ie n .IP "\fI\fI$model\fI\fR\->\fBset_n\fR(\fI\fI$n\fI\fR);" 4
.el .IP "\fI\f(CI$model\fI\fR\->\fBset_n\fR(\fI\f(CI$n\fI\fR);" 4
.IX Item "$model->set_n($n);"
Set order of n\-gram model to use, corresponding to a history size of \fI\f(CI$n\fI\fR\-1 characters.
\&\fI\f(CI$n\fI\fR may not be larger than the order the model has been initialised and trained with.
.ie n .IP "\fI\fI$model\fI\fR\->\fBnormalize\fR(\fI\fI$mode\fI\fR);" 4
.el .IP "\fI\f(CI$model\fI\fR\->\fBnormalize\fR(\fI\f(CI$mode\fI\fR);" 4
.IX Item "$model->normalize($mode);"
Set text normalisation mode to one of the following values, which specify
increasingly heavy normalisation.
.Sp
.Vb 4
\&    0 = minimal normalisation (only whitespace and control characters)
\&    1 = map all high\-bit characters (outside ASCII range) to "~"
\&    2 = map ASCII letters to "a" for vowels and "t" for consonants, digits to "0"
\&    3 = non\-lexical model, maps all ASCII letters to "a" and all digits to "0"
.Ve
.ie n .IP "\fI\fI$model\fI\fR\->\fBtrain\fR(\fI\fI$text\fI\fR);" 4
.el .IP "\fI\f(CI$model\fI\fR\->\fBtrain\fR(\fI\f(CI$text\fI\fR);" 4
.IX Item "$model->train($text);"
.PD 0
.ie n .IP "\fI\fI$model\fI\fR\->\fBtrain\fR(\fI\fI$text\fI\fR, \fI\fI$diff_text\fI\fR);" 4
.el .IP "\fI\f(CI$model\fI\fR\->\fBtrain\fR(\fI\f(CI$text\fI\fR, \fI\f(CI$diff_text\fI\fR);" 4
.IX Item "$model->train($text, $diff_text);"
.PD
Train n\-gram model on \fI\f(CI$text\fI\fR.  The second form performs \fBdifferential
training\fR, where n\-gram counts for \fI\f(CI$diff_text\fI\fR are subtracted from those of
\&\fI\f(CI$text\fI\fR.  If \fI\f(CI$diff_text\fI\fR is a subset of \fI\f(CI$text\fI\fR, this feature approximates
training on the difference of the two texts (which may be difficult to compute
in some cases).
.ie n .IP "\fI\fI$bits_per_char\fI\fR = \fI\fI$model\fI\fR\->\fBcross_entropy\fR(\fI\fI$text\fI\fR [, \fI\fI$n\fI\fR]);" 4
.el .IP "\fI\f(CI$bits_per_char\fI\fR = \fI\f(CI$model\fI\fR\->\fBcross_entropy\fR(\fI\f(CI$text\fI\fR [, \fI\f(CI$n\fI\fR]);" 4
.IX Item "$bits_per_char = $model->cross_entropy($text [, $n]);"
Estimate cross-entropy, i.e. the average surprise per character measured in bits,
for \fI\f(CI$model\fI\fR on the string \fI\f(CI$text\fI\fR.  The optional argument \fI\f(CI$n\fI\fR specifies the 
order of the n\-gram model to use (up to the order used in training).
.ie n .IP "\fI\fI$log_p\fI\fR = \fI\fI$model\fI\fR\->\fBlog_prob\fR(\fI\fI$text\fI\fR [, \fI\fI$n\fI\fR]);" 4
.el .IP "\fI\f(CI$log_p\fI\fR = \fI\f(CI$model\fI\fR\->\fBlog_prob\fR(\fI\f(CI$text\fI\fR [, \fI\f(CI$n\fI\fR]);" 4
.IX Item "$log_p = $model->log_prob($text [, $n]);"
Calculate the base\-2 logarithm of the probability of \fI\f(CI$text\fI\fR according to
\&\fI\f(CI$model\fI\fR.  The optional argument \fI\f(CI$n\fI\fR specifies the order of the n\-gram
model to use (up to the order used in training).
.ie n .IP "\fI\fI$cp\fI\fR = \fI\fI$model\fI\fR\->\fBcp\fR(\fI\fI$ngram\fI\fR);" 4
.el .IP "\fI\f(CI$cp\fI\fR = \fI\f(CI$model\fI\fR\->\fBcp\fR(\fI\f(CI$ngram\fI\fR);" 4
.IX Item "$cp = $model->cp($ngram);"
.PD 0
.ie n .IP "\fI\fI$log_cp\fI\fR = \fI\fI$model\fI\fR\->\fBlog_cp\fR(\fI\fI$ngram\fI\fR);" 4
.el .IP "\fI\f(CI$log_cp\fI\fR = \fI\f(CI$model\fI\fR\->\fBlog_cp\fR(\fI\f(CI$ngram\fI\fR);" 4
.IX Item "$log_cp = $model->log_cp($ngram);"
.PD
Estimate conditional probability (\fBcp\fR) or its base\-2 logarithm (\fBlog_cp\fR)
for the n\-gram \fI\f(CI$ngram\fI\fR, with interpolation and smoothing applied.
.Sp
The conditional probability of an n\-gram is defined as the probability of the
last character given the \*(L"history\*(R" of the previous n\-1 characters.
.SH "AUTHOR"
.IX Header "AUTHOR"
Stefan Evert \f(CW\*(C`<stefan.evert@uos.de>\*(C'\fR
.SH "COPYRIGHT & LICENSE"
.IX Header "COPYRIGHT & LICENSE"
Copyright (C) 2008 by Stefan Evert, all rights reserved.
.PP
This program is free software; you can redistribute it and/or modify it
under the same terms as Perl itself.
